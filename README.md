# Regularization
I use a one-layer neural network trained on the MNIST dataset to give an intuition for how common regularization techniques affect learning.

Check out my [blog post](https://greydanus.github.io/2016/09/05/regularization/)

View the notebooks:

* [No regularization](https://nbviewer.jupyter.org/github/greydanus/regularization/blob/master/no_regularization.ipynb)
* [Dropout](https://nbviewer.jupyter.org/github/greydanus/regularization/blob/master/dropout.ipynb)
* [Gaussian weight regularization](https://nbviewer.jupyter.org/github/greydanus/regularization/blob/master/gaussian_weight.ipynb)
* [L2 regularization](https://nbviewer.jupyter.org/github/greydanus/regularization/blob/master/l2_regularization.ipynb)
* [Weight normalization](https://nbviewer.jupyter.org/github/greydanus/regularization/blob/master/weight_normalization.ipynb)
